{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-28T13:31:28.390404Z",
     "iopub.status.busy": "2026-01-28T13:31:28.389967Z",
     "iopub.status.idle": "2026-01-28T13:31:28.435244Z",
     "shell.execute_reply": "2026-01-28T13:31:28.434030Z",
     "shell.execute_reply.started": "2026-01-28T13:31:28.390357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1241, 81)\n",
      "Test shape: (219, 80)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/holberton-baku-ml-2-housing-prices/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/holberton-baku-ml-2-housing-prices/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:31:28.437877Z",
     "iopub.status.busy": "2026-01-28T13:31:28.437397Z",
     "iopub.status.idle": "2026-01-28T13:31:47.789814Z",
     "shell.execute_reply": "2026-01-28T13:31:47.789044Z",
     "shell.execute_reply.started": "2026-01-28T13:31:28.437839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Training XGBoost...\n",
      "âœ… XGBoost Trained!\n",
      "ðŸ“ˆ Training Linear Model...\n",
      "âœ… Linear Model Trained!\n",
      "ðŸ¥£ Blending Models...\n",
      "ðŸš€ Success! submission_super_blend.csv created (No missing files!)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# ==========================================\n",
    "# PART 1: TRAIN XGBOOST (Tree Model)\n",
    "# ==========================================\n",
    "print(\"ðŸŒ² Training XGBoost...\")\n",
    "\n",
    "# 1. Clean Data for Trees (Simple, No Polynomials)\n",
    "def clean_data_tree(df):\n",
    "    df_clean = df.copy()\n",
    "    # Basic Map\n",
    "    quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "    for col in ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].fillna('None').map(quality_map).fillna(0)\n",
    "            \n",
    "    # Fill NAs\n",
    "    df_clean.fillna(0, inplace=True) # Trees handle 0s fine\n",
    "    \n",
    "    # Feature Engineering (Simple)\n",
    "    if 'YearBuilt' in df_clean.columns:\n",
    "        df_clean['HouseAge'] = 2010 - df_clean['YearBuilt']\n",
    "    df_clean['TotalSF'] = df_clean['TotalBsmtSF'] + df_clean['1stFlrSF'] + df_clean['2ndFlrSF']\n",
    "    df_clean['TotalBath'] = df_clean['FullBath'] + (0.5 * df_clean['HalfBath']) + df_clean['BsmtFullBath'] + (0.5 * df_clean['BsmtHalfBath'])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# 2. Prepare & Train\n",
    "train_tree = clean_data_tree(train)\n",
    "test_tree = clean_data_tree(test)\n",
    "\n",
    "train_tree = pd.get_dummies(train_tree)\n",
    "test_tree = pd.get_dummies(test_tree)\n",
    "train_tree, test_tree = train_tree.align(test_tree, join='left', axis=1)\n",
    "test_tree = test_tree.fillna(0)\n",
    "\n",
    "X_tree = train_tree.drop(['Id', 'SalePrice'], axis=1)\n",
    "y_tree = np.log1p(train_tree['SalePrice'])\n",
    "X_test_tree = test_tree.drop(['Id', 'SalePrice'], axis=1)\n",
    "\n",
    "xgb = XGBRegressor(n_estimators=3000, learning_rate=0.01, max_depth=4, \n",
    "                   min_child_weight=1, gamma=0, subsample=0.7, \n",
    "                   colsample_bytree=0.7, n_jobs=-1, random_state=42)\n",
    "xgb.fit(X_tree, y_tree)\n",
    "\n",
    "# 3. Get Predictions (XGBoost)\n",
    "log_xgb_preds = xgb.predict(X_test_tree)\n",
    "xgb_preds = np.expm1(log_xgb_preds)\n",
    "print(\"âœ… XGBoost Trained!\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PART 2: TRAIN LINEAR (ElasticNet)\n",
    "# ==========================================\n",
    "print(\"ðŸ“ˆ Training Linear Model...\")\n",
    "\n",
    "# 1. Clean Data for Linear (Needs Polynomials!)\n",
    "def clean_data_linear(df, is_train=True):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Standard Mappings\n",
    "    quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "    ord_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "    for col in ord_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].fillna('None').map(quality_map).fillna(0)\n",
    "\n",
    "    # Fill NAs\n",
    "    numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    cols_to_zero = [c for c in numeric_cols if c not in ['Id', 'SalePrice']]\n",
    "    df_clean[cols_to_zero] = df_clean[cols_to_zero].fillna(0)\n",
    "    \n",
    "    # Feature Engineering (Polynomials)\n",
    "    if 'YearBuilt' in df_clean.columns:\n",
    "        df_clean['HouseAge'] = 2010 - df_clean['YearBuilt']\n",
    "    \n",
    "    df_clean['TotalSF'] = (df_clean['TotalBsmtSF'] + df_clean['1stFlrSF'] + df_clean['2ndFlrSF'])\n",
    "    \n",
    "    # Important Polynomials for Linear\n",
    "    if 'OverallQual' in df_clean.columns:\n",
    "         df_clean['OverallQual_Sq'] = df_clean['OverallQual'] ** 2\n",
    "         df_clean['TotalSF_Sq'] = df_clean['TotalSF'] ** 2\n",
    "         df_clean['OverallQual_Cu'] = df_clean['OverallQual'] ** 3\n",
    "         if 'HouseAge' in df_clean.columns:\n",
    "             df_clean['Qual_Age'] = df_clean['OverallQual'] * (1 / (df_clean['HouseAge'] + 1))\n",
    "             \n",
    "    if is_train and 'GrLivArea' in df_clean.columns:\n",
    "        df_clean = df_clean[df_clean['GrLivArea'] < 4000]\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# 2. Prepare & Train\n",
    "train_lin = clean_data_linear(train, is_train=True)\n",
    "test_lin = clean_data_linear(test, is_train=False)\n",
    "\n",
    "train_lin = pd.get_dummies(train_lin)\n",
    "test_lin = pd.get_dummies(test_lin)\n",
    "train_lin, test_lin = train_lin.align(test_lin, join='left', axis=1)\n",
    "test_lin = test_lin.fillna(0)\n",
    "\n",
    "X_lin = train_lin.drop(['Id', 'SalePrice'], axis=1)\n",
    "y_lin = np.log1p(train_lin['SalePrice'])\n",
    "X_test_lin = test_lin.drop(['Id', 'SalePrice'], axis=1)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled_lin = scaler.fit_transform(X_lin)\n",
    "X_test_scaled_lin = scaler.transform(X_test_lin)\n",
    "\n",
    "elastic = ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9, 1], cv=5, max_iter=50000, n_jobs=-1)\n",
    "elastic.fit(X_scaled_lin, y_lin)\n",
    "\n",
    "# 3. Get Predictions (Linear)\n",
    "linear_preds = np.expm1(elastic.predict(X_test_scaled_lin))\n",
    "print(\"âœ… Linear Model Trained!\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PART 3: BLEND & SUBMIT\n",
    "# ==========================================\n",
    "print(\"ðŸ¥£ Blending Models...\")\n",
    "\n",
    "# Weighted Average: 60% Linear + 40% XGBoost\n",
    "final_blend = (0.60 * linear_preds) + (0.40 * xgb_preds)\n",
    "\n",
    "submission = pd.DataFrame({'Id': test['Id'], 'SalePrice': final_blend})\n",
    "submission.to_csv('submission_super_blend.csv', index=False)\n",
    "\n",
    "print(\"ðŸš€ Success! submission_super_blend.csv created (No missing files!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T13:31:47.791641Z",
     "iopub.status.busy": "2026-01-28T13:31:47.791330Z",
     "iopub.status.idle": "2026-01-28T13:31:48.670242Z",
     "shell.execute_reply": "2026-01-28T13:31:48.667406Z",
     "shell.execute_reply.started": "2026-01-28T13:31:47.791609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating MAE on FRESH Data...\n",
      "Average MAE: $14,807.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# --- 1. REFRESH DATA (Crucial Step!) ---\n",
    "# We force Python to use the NEW cleaning function you just wrote\n",
    "# to create fresh X and y variables.\n",
    "train_fresh = clean_data_linear(train, is_train=True) # <--- Using your latest function\n",
    "train_fresh = pd.get_dummies(train_fresh)\n",
    "train_fresh = train_fresh.fillna(0)\n",
    "\n",
    "X_fresh = train_fresh.drop(['Id', 'SalePrice'], axis=1)\n",
    "y_fresh = np.log1p(train_fresh['SalePrice'])\n",
    "\n",
    "# --- 2. SETUP SIMULATION ---\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mae_scores = []\n",
    "\n",
    "X_vals = X_fresh.values \n",
    "y_vals_log = y_fresh.values\n",
    "\n",
    "print(\"Calculating MAE on FRESH Data...\")\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_vals):\n",
    "    # A. Split\n",
    "    X_t, X_v = X_vals[train_idx], X_vals[val_idx]\n",
    "    y_t_log = y_vals_log[train_idx]\n",
    "    y_v_real = np.expm1(y_vals_log[val_idx])\n",
    "    \n",
    "    # B. Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_t_scaled = scaler.fit_transform(X_t)\n",
    "    X_v_scaled = scaler.transform(X_v)\n",
    "    \n",
    "    # C. Train (Ridge)\n",
    "    model = Ridge(alpha=20)\n",
    "    model.fit(X_t_scaled, y_t_log)\n",
    "    \n",
    "    # D. Predict\n",
    "    pred_log = model.predict(X_v_scaled)\n",
    "    pred_dollar = np.expm1(pred_log)\n",
    "    \n",
    "    # E. Score\n",
    "    score = mean_absolute_error(y_v_real, pred_dollar)\n",
    "    mae_scores.append(score)\n",
    "\n",
    "print(f\"Average MAE: ${np.mean(mae_scores):,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15019769,
     "sourceId": 126106,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
